### Qwen2.5-{0.5,1.5,3,7,14,32}B-Instruct ###
PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-0.5B-Instruct,tokenizer=Qwen/Qwen2.5-0.5B-Instruct,dtype=bfloat16,tensor_parallel_size=1 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_0.5 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"
PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-1.5B-Instruct,tokenizer=Qwen/Qwen2.5-1.5B-Instruct,dtype=bfloat16,tensor_parallel_size=1 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_1.5 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"
#repeat only 1.5 twice
PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-1.5B-Instruct,tokenizer=Qwen/Qwen2.5-1.5B-Instruct,dtype=bfloat16,tensor_parallel_size=1 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_1.5_seeded --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0,seed=42"
PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-3B-Instruct,tokenizer=Qwen/Qwen2.5-3B-Instruct,dtype=bfloat16,tensor_parallel_size=1 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_3 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"
PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,tokenizer=Qwen/Qwen2.5-7B-Instruct,dtype=bfloat16,tensor_parallel_size=1 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_7 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"
#PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-14B-Instruct,tokenizer=Qwen/Qwen2.5-14B-Instruct,dtype=bfloat16,tensor_parallel_size=4 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_14 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"
#PROCESSOR='gpt-4o-mini' lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-32B-Instruct,tokenizer=Qwen/Qwen2.5-32B-Instruct,dtype=bfloat16,tensor_parallel_size=4 --tasks openai_math_agg64 --batch_size auto --apply_chat_template --output_path ./data/lm_eval/qwen_32 --log_samples --gen_kwargs "max_gen_toks=8192,temperature=1.0"

